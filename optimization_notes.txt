Overview
--------

This file includes some general notes about outstanding optimizations
that can still be made.

We can break the operations of the innermost loop of calc_vsf_props
function into 3 distinct steps:

1. distance and absolute velocity difference calculation
2. identification of distance bin index
3. statistic update within the appropriate distance bin

For the range of arguments that calc_vsf_props currently be accepted,
only the first step can be vectorized in the general case. The
branching and introduced in the remaining parts prevents vectorization
in the other cases.

As an aside, this shows the potential advantages of using an algorithm
in which approximate distances are computed using kd-trees or
octrees. Using such approximate distances can allow you to compute the
statistics in a single distance bin (or a very small number of
distance bins) at a time. This essentially removes all branching and
facillitates vectorization of the entire innermost loop. However, such
an approach is complicated to implement and its unclear what
conditions are necessary for the time savings within the loop to
compensate for the overhead of the approximate distance calculations.


Below I'll walk through some thoughts for vectorizing each step. Note
that this discussion is mostly hypothetical (I mostly wanted to
record my ideas). Before any optimizations are made, the code should
probably be profiled.

distance and absolute velocity difference calculation
-----------------------------------------------------

At the time of writing, it seems unlikely that the distance and
absolute velocity difference are actually vectorized (obviously this
should be checked).

Broadly speaking vectorizing this step can be vectorized by computing
distances/velocity differences in batches. These results would be stored
in a small buffer (the exact size) and after the remaining steps would be
performed after the buffer is filled. In detail, there are a few other
factors to be considered:

- I don't think the c standard library's sqrt operation vectorizes that
  well (I'm not really sure). However SIMD intrinsics definitely exist
  for doing this. In practice, we just compute the squared distance and
  squared velocity difference in this step and defer any sqrt
  evaluations to the next step (which we might not be able to
  vectorize anyway).

- We might need to explicitly unroll the loops that fill the buffer to
  demonstrate to coax the compiler into producing vectorized code. We
  also need to be mindful of C++'s aliasing rules if we factor this out
  into a separate function.

- We need to think about how to fill the buffer at the start and end
  of the innermost loop. The difference in the loop bounds when the
  vsf is computed for a single collection of points compared to the
  loop bounds when computing the vsf for two pairs of points is
  definitely relevant. This might take additional work if we need to
  explicitly unroll the loops that fill the buffer.

- Currently the positions and velocities are organized such that the
  the dimension changes along the slow indexing axis. For this
  approach, this memory layout should be faster (the other layout
  requires shuffles). However some work is needed to:
  - make sure that the start of each dimension has the appropriate
    alignment (i.e. the array probably needs to be padded or we need
    to allocate separate arrays for each dimension)
  - inform the compiler of the alignment of each dimension. This might
    involve writing separate code for 1d distances, 2d distances, and
    3d distances.

As a short term solution, (without vectorization) it might be faster
to change the memory layout order to match scipy (i.e. the dimensions
vary over the fast axis).

identification of distance bin index
------------------------------------

Currently, we precompute the bin edges in terms of the squared distances
so that we can avoid a sqrt operation in the innermost loop (i.e. we never
need to know the actual distance, just knowning the squared distance is
sufficient). Crude benchmarking indicates that this optimization speeds
the code up by ~10% when `statistic = "variance"`.

In the case of having bins with a constant linear spacing, it's actually
possible to vectorize this operation (if the first step has been modified
to operate in batches). However, doing so would involve some refactoring
to take the sqrt of the squared distances.

The current algorithm uses linear bisection to indentify the correct bin.
In the case of having relatively few bins, it might be worth changing the
search algorithm to avoid branches. This involves using linear search (that
always checks every bin) or a branchless bisection search (a search that
always goes through the maximum number of iterations). Such a linear search
could actually be vectorized. Note that the changes discussed in this last
paragraph probably won't provide enough performance benefit to be warranted

statistic update within the appropriate distance bin
----------------------------------------------------

There aren't many optimizations to discuss for this step (due to the
branching that directly preceedes this step).

However, in the event that the first step is optimized to employ
vectorization, it might be worth considering the implementation of
these algorithms to use single precision floating point values and
compensated summation (aka Kahan summation). The thinking here is
that the use of single precision floats alongside compensated summation
MIGHT provide a speedup (the speedup comes from the first step) over
double precision without sacrificing too much accuracy. However, this
definitely seems like a bit of a stretch.
